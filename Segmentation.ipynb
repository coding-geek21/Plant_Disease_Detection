{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmIQqx63f3AH"
      },
      "source": [
        "### Importing all the required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kxHfaw3f3AU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Open Source Computer Vision Library\n",
        "import cv2 \n",
        "\n",
        "# keeps track of the objects it has already serialized, so that later references to the same object wonâ€™t be serialized again\n",
        "import pickle\n",
        "\n",
        "# provides a portable way of using operating system dependent functionality. \n",
        "import os\n",
        "from os import listdir \n",
        "from os.path import isfile, join\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split # Split arrays or matrices into random train and test subsets\n",
        "from sklearn.model_selection import cross_val_score # Evaluate a score by cross-validation\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler # Standardize features by removing the mean and scaling to unit variance\n",
        "from sklearn.linear_model import SGDClassifier # Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n",
        "\n",
        "from sklearn.metrics import accuracy_score # Accuracy classification score.\n",
        "from sklearn.metrics import confusion_matrix #Compute confusion matrix to evaluate the accuracy of a classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FucnRcxhf3Aa"
      },
      "source": [
        "### Reading Images from a folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0Dd0o11f3Ab"
      },
      "source": [
        "mypath='C:/Users/V Sundar/Downloads'# change the path to read corresponding folders.\n",
        "onlyfiles = [ f for f in listdir(mypath) if isfile(join(mypath,f)) ]\n",
        "images = np.empty(len(onlyfiles), dtype=object)\n",
        "resized_image = np.empty(len(onlyfiles), dtype=object)\n",
        "for n in range(0, len(onlyfiles)):\n",
        "    \n",
        "    images[n] = cv2.imread( join(mypath,onlyfiles[n]) ) # Reading all the images from folder and storing in numpy array.\n",
        "    resized_image[n] = cv2.resize(images[n], (100,100)) # Resizing all the images into 100X100 pixels.\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cR60dLDf3Ad"
      },
      "source": [
        "print(type(resized_image)) # to check the type.\n",
        "print(resized_image.shape) # check the no of images present in folder."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5U3XgiVcf3Ag"
      },
      "source": [
        "### Segmentation by K-Mean Clustering.\n",
        "K-Means is a least-squares partitioning method that divide a collection of objects into K groups. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuUZVIG4f3Ai"
      },
      "source": [
        "### cv2.kmeans() function in OpenCV for data clustering\n",
        "#### Input Parameters\n",
        "\n",
        "1. **samples** : It should be of np.float32 data type, and each feature should be put in a single column.<br>\n",
        "<br>\n",
        "2. **nclusters(K)** : Number of clusters required at end<br>\n",
        "<br>\n",
        "3. **criteria** : It is the iteration termination criteria. When this criteria is satisfied, algorithm iteration stops. Actually, it should be a tuple of 3 parameters. They are ( type, max_iter, epsilon ):<br>\n",
        "\n",
        "3.1. type of termination criteria : It has 3 flags as below:<br>\n",
        "**cv2.TERM_CRITERIA_EPS** - stop the algorithm iteration if specified accuracy, epsilon, is reached. <br>**cv2.TERM_CRITERIA_MAX_ITER** - stop the algorithm after the specified number of iterations, max_iter. <br>\n",
        "**cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER** - stop the iteration when any of the above condition is met.<br>\n",
        "\n",
        "\n",
        "3.2. **max_iter** - An integer specifying maximum number of iterations.<br>\n",
        "\n",
        "3.3. **epsilon** - Required accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH0W7Mekf3Am"
      },
      "source": [
        "# Initialize all the arrays as empty.\n",
        "gray = np.empty(len(images), dtype=object)\n",
        "Y = np.empty(len(images), dtype=object)\n",
        "label = np.empty(len(images), dtype=object)\n",
        "res = np.empty(len(images), dtype=object)\n",
        "res2 = np.empty(len(images), dtype=object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GhJBYKWf3An"
      },
      "source": [
        "\n",
        "There are 3 features, say, R,G,B. So we need to reshape the image to an array of Mx3 size (M is number of pixels in image). And after the clustering, we apply centroid values (it is also R,G,B) to all pixels, such that resulting image will have specified number of colors. And again we need to reshape it back to the shape of original image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXvBpNnjf3Ao"
      },
      "source": [
        "for i in range(0,n):\n",
        "    Y[i]=resized_image[i].reshape((-1,3))\n",
        "    Y[i]= np.float32(Y[i])\n",
        "    \n",
        "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "K=4 # No of clusters=4\n",
        "\n",
        "for i in range(0,n):    \n",
        "    ret,label[i],center=cv2.kmeans(Y[i],K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)# Using cv2.Kmeans() Function\n",
        "    \n",
        "center = np.uint8(center) #  array of centers of clusters.\n",
        "\n",
        "for i in range(0,n):    \n",
        "    res[i] = center[label[i].flatten()]\n",
        "    res2[i] = res[i].reshape((resized_image[i].shape))\n",
        "    gray[i] = cv2.cvtColor(res2[i], cv2.COLOR_BGR2GRAY)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgnX6236f3Ap"
      },
      "source": [
        "# Flatten the segmented array.\n",
        "for i in range(0,n):\n",
        "        res2[i]=res2[i].flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MeJYNLef3Aq",
        "outputId": "6b621303-14eb-44a6-a533-79ee67ca8df3"
      },
      "source": [
        "# Variants of numpy.stack function to stack so as to make a single array vertically.\n",
        "abc = np.empty(30000) # 100*100*3\n",
        "for i in range(961):# Size of folder-1\n",
        "    abc = np.vstack((abc, res2[i]))\n",
        "abc  = abc[1:,:]\n",
        "abc.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(961, 30000)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Seg8pE2f3Ar"
      },
      "source": [
        "# Convert ndarray into Dataframe.\n",
        "df16=pd.DataFrame(data=abc)\n",
        "df16['label']=16 # Label the images whichever class it may belong to."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYPCrg0ff3Ar"
      },
      "source": [
        "Doing the above procedure for 9 folders and appending it into single Dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEq1U429f3As"
      },
      "source": [
        "with open('dfc16-24', 'rb') as pickle_file:\n",
        "    df = pickle.load(pickle_file)# list_of_sent of summary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww_kBDWQf3As"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze8Yc9AKf3At"
      },
      "source": [
        "#### Slicing the dataframe and taking 5000 points for creating model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF_Te32Qf3At"
      },
      "source": [
        "X=df.iloc[:5000,:30000].values\n",
        "y=df.iloc[:5000,30000].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i-Ct6Rxf3Au"
      },
      "source": [
        "x.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR5D34Ovg2OD"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKksw-wuf3Av"
      },
      "source": [
        "s=StandardScaler()\n",
        "X=s.fit_transform(X)# Perform column Standardization "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0S6Oac7f3Av"
      },
      "source": [
        "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2) # Split the dataset into train and test as 80-20 split"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}